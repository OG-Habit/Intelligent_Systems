{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb6795",
   "metadata": {},
   "source": [
    "# ECG\n",
    "---\n",
    "- An electrocardiogram (ECG) is a simple, non-invasive test that records the electrical activity of the heart.<br>\n",
    "- An ECG can help diagnose certain heart conditions, including abnormal heart rhythms and coronary heart disease (heart attack and angina).<br>\n",
    "- A doctor may recommend an ECG if you are experiencing symptoms like chest pain, breathlessness, dizziness, fainting or a feeling of your heart racing, fluttering, thumping or pounding in your chest (palpitations).<br>\n",
    "- An ECG can also help monitor how treatments for a heart condition, like medicines or implantable cardiac devices, are working.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd68fd",
   "metadata": {},
   "source": [
    "# Diseases that are diagnosed with ECG\n",
    "---\n",
    "- Abnormal heart rhythms (arrhythmia).\n",
    "- Heart inflammation (pericarditis or myocarditis).\n",
    "- Ennlargement of the heart walls or viens (Cardiomyopathy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a39de7",
   "metadata": {},
   "source": [
    "# Step 1: Read the dataset\n",
    "---\n",
    "### About the Dataset\n",
    "---\n",
    "https://www.timeseriesclassification.com/description.php?Dataset=ECG5000\n",
    "\n",
    "- The original dataset for \"ECG5000\" is a 20-hour long ECG downloaded from Physionet. The name is BIDMC Congestive Heart Failure Database(chfdb) and it is record \"chf07\". It was originally published in \"Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. Circulation 101(23)\". The data was pre-processed in two steps: (1) extract each heartbeat, (2) make each heartbeat equal length using interpolation. This dataset was originally used in paper \"A general framework for never-ending learning from time series streams\", DAMI 29(6). After that, 5,000 heartbeats were randomly selected. The patient has severe congestive heart failure and the class values were obtained by automated annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c93cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv', header=None)\n",
    "raw_data = dataframe.values\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a20d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45282aa0",
   "metadata": {},
   "source": [
    "# Separate the target variable from the features.\n",
    "---\n",
    "- Normal rhythms, which are labeled in this dataset as 1. \n",
    "- Abnornmal heart rhythms are labeled as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82205e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last element contains the labels\n",
    "labels = raw_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the normal rhythms from the abnormal rhythms.\n",
    "# Train the autoencoder using only the normal rhythms.\n",
    "# Test the autoencoder using only the abnormal rhythms\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4e866",
   "metadata": {},
   "source": [
    "# Separate the data to test and train\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The other data points are the electrocadriogram data\n",
    "data = raw_data[:, 0:-1]\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=21\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174fa1f",
   "metadata": {},
   "source": [
    "# Normalize the dataset\n",
    "---\n",
    "<font color=red><b>QUESTION</b></font>\n",
    "- When to use normalization as oppose to standardization?\n",
    "    - If there are different scales for the data and the machine learning algorithm used is sensitve to the scaling of the features, normalization is recommeneded to use over standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdebce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(type, train, test):\n",
    "    train_trans = None\n",
    "    test_trans = None\n",
    "    sc = None\n",
    "    if type == \"NOR\":\n",
    "        min_val = tf.reduce_min(train)\n",
    "        max_val = tf.reduce_max(train)\n",
    "\n",
    "        train = (train - min_val) / (max_val - min_val)\n",
    "        test = (test - min_val) / (max_val - min_val)\n",
    "\n",
    "        train_trans = tf.cast(train, tf.float32)\n",
    "        test_trans = tf.cast(test, tf.float32)\n",
    "    elif type == \"STA\":\n",
    "        sc = StandardScaler()\n",
    "        train = sc.fit_transform(train)\n",
    "        test = sc.transform(test)\n",
    "\n",
    "        train_trans = tf.cast(train, tf.float32)\n",
    "        test_trans = tf.cast(test, tf.float32)\n",
    "    elif type == \"LOG\":\n",
    "        # Log transformation\n",
    "        epsilon = 1e-8  # Small constant to avoid log(0)\n",
    "        train = np.log1p(train + epsilon)\n",
    "        test = np.log1p(test + epsilon)\n",
    "\n",
    "        train_trans = tf.cast(train, tf.float32)\n",
    "        test_trans = tf.cast(test, tf.float32)\n",
    "    elif type == \"MIN\":\n",
    "        sc = MinMaxScaler()\n",
    "        train = sc.fit_transform(train)\n",
    "        test = sc.transform(test)\n",
    "\n",
    "        train_trans = tf.cast(train, tf.float32)\n",
    "        test_trans = tf.cast(test, tf.float32)\n",
    "\n",
    "    return train_trans, test_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaed647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "train_data, test_data = transform('NOR', train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.astype(bool)\n",
    "test_labels = test_labels.astype(bool)\n",
    "\n",
    "normal_train_data = train_data[train_labels]\n",
    "normal_test_data = test_data[test_labels]\n",
    "\n",
    "anomalous_train_data = train_data[~train_labels]\n",
    "anomalous_test_data = test_data[~test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efb167",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.plot(np.arange(140), normal_train_data[0])\n",
    "plt.title(\"A Normal ECG\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dadfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.plot(np.arange(140), anomalous_train_data[0])\n",
    "plt.title(\"An Anomalous ECG\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52cfdbf",
   "metadata": {},
   "source": [
    "# Create the architecture of the autoencoder\n",
    "---\n",
    "- What are the parts and the purpose of each part of an autoencoder?\n",
    "    - Encoder: Compresses input and produces the code\n",
    "    - Code: Compressed version of the input\n",
    "    - Decoder: Reconstructs the input from the compressed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector(Model):\n",
    "  def __init__(self):\n",
    "    super(AnomalyDetector, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(64, activation=\"relu\"),\n",
    "      layers.Dense(16, activation=\"relu\"),\n",
    "      layers.Dense(16, activation=\"relu\"),\n",
    "      layers.Dense(8, activation=\"relu\"),\n",
    "      layers.Dropout(0.001)])\n",
    "\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(8, activation=\"relu\"),\n",
    "      layers.Dense(16, activation=\"relu\"),\n",
    "      layers.Dense(32, activation=\"relu\"),\n",
    "      layers.Dense(64, activation=\"relu\"),\n",
    "      layers.Dense(140, activation=\"sigmoid\")])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = AnomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cf2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mae')\n",
    "# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc6571a",
   "metadata": {},
   "source": [
    "# Train your autoencoder architecture\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(normal_train_data, normal_train_data, \n",
    "          epochs=20, \n",
    "          batch_size=512,\n",
    "          validation_data=(test_data, test_data),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b66a68b",
   "metadata": {},
   "source": [
    "# Plot the errors\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19392a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c8caa",
   "metadata": {},
   "source": [
    "# Question\n",
    "---\n",
    "- In reality trainnig loss should always be lower than validation loss, why?\n",
    "    - It should always be lower than validation loss because it shows that the model was able to learn properly the characteristics and patterns of the data for it to perform well on the validation data or new and unseen data. But, it should not be too low since it would indicate overfitting.\n",
    "- What happens if validation loss is lower than the training loss?\n",
    "    - If validation loss is lower than the training loss, it could mean that the model was unable to learn properly the underlying patterns and characteristics of the testing data for it to adapt to new and unseen data, i.e., validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf63151",
   "metadata": {},
   "source": [
    "# Performance Testing from Training Data\n",
    "---\n",
    "- We can use our trained autoencoders to reconstruct the ECG signals.\n",
    "- Ideally out reconstructed signals shold look exactly the same as the training signal but in reality it is not due to some variation in the data.\n",
    "- We then create a \"reconstruction error\" This is an area that determines how close or how far the reconstructed data is to the  datatraining.\n",
    "- For medical settings the reconstruction error should be lower than 1 standard deviation of the training data.\n",
    "\n",
    "#### Detecting anomalies\n",
    "---\n",
    "- Detect anomalies by calculating whether the reconstruction loss is greater than a fixed threshold. \n",
    "- Determine the mean average error for normal examples from the training set, then classify future examples as anomalous if the reconstruction error is higher than one standard deviation from the training set.\n",
    "- Choose a threshold value that is one standard deviations above the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb291e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = autoencoder.encoder(normal_test_data).numpy()\n",
    "decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "\n",
    "plt.plot(normal_test_data[0], 'b')\n",
    "plt.plot(decoded_data[0], 'r')\n",
    "plt.fill_between(np.arange(140), decoded_data[0], normal_test_data[0], color='lightcoral')\n",
    "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = autoencoder.predict(normal_train_data)\n",
    "train_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n",
    "\n",
    "plt.hist(train_loss[None,:], bins=50)\n",
    "plt.xlabel(\"Train loss\")\n",
    "plt.ylabel(\"No of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc85ba3",
   "metadata": {},
   "source": [
    "# Generating Recommendations for Diagnosis (THIS IS <font color=red>NOT A DIAGNOSIS!</font>)\n",
    "---\n",
    "- Once the construction error is confirmed to be lower than 1 standard deviation form the training set, we can use the same concept towards the testing dataset.\n",
    "- Plot the testing dataset, and reconstruct the signal from the decoded layer of the autoencoders, and generate a reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = autoencoder.encoder(anomalous_test_data).numpy()\n",
    "decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "\n",
    "plt.plot(anomalous_test_data[0], 'b')\n",
    "plt.plot(decoded_data[0], 'r')\n",
    "plt.fill_between(np.arange(140), decoded_data[0], anomalous_test_data[0], color='lightcoral')\n",
    "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50912cbc",
   "metadata": {},
   "source": [
    "If you examine the reconstruction error for the anomalous examples in the test set, you'll notice most have greater reconstruction error than the threshold. By varing the threshold, you can adjust the precision and recall of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cacba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = autoencoder.predict(anomalous_test_data)\n",
    "test_loss = tf.keras.losses.mae(reconstructions, anomalous_test_data)\n",
    "\n",
    "plt.hist(test_loss[None, :], bins=50)\n",
    "plt.xlabel(\"Test loss\")\n",
    "plt.ylabel(\"No of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5245a",
   "metadata": {},
   "source": [
    "Classify an ECG as an anomaly if the reconstruction error is greater than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f743a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, threshold):\n",
    "  reconstructions = model(data)\n",
    "  loss = tf.keras.losses.mae(reconstructions, data)\n",
    "  return tf.math.less(loss, threshold)\n",
    "\n",
    "def print_stats(predictions, labels):\n",
    "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
    "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
    "  print(\"Recall = {}\".format(recall_score(labels, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(autoencoder, test_data, threshold)\n",
    "print_stats(preds, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98169b9",
   "metadata": {},
   "source": [
    "# Coding Experiments\n",
    "- Holding the autoencoder architecture constant. What would happen to the performance if you chosed other data transformation? Experiment using standardization, and log transformation, minmax scaler.\n",
    "    - Normalization\n",
    "        - Accuracy = 0.946\n",
    "        - Precision = 0.994140625\n",
    "        - Recall = 0.9089285714285714\n",
    "    - Standardization\n",
    "        - Accuracy = 0.643\n",
    "        - Precision = 0.6236297198538368\n",
    "        - Recall = 0.9142857142857143\n",
    "    - Log transformation - error cause of NaN values\n",
    "        - Accuracy = NaN\n",
    "        - Precision = NaN\n",
    "        - Recall = NaN\n",
    "    - Minmax scaler \n",
    "        - Accuracy = 0.946\n",
    "        - Precision = 0.9922178988326849\n",
    "        - Recall = 0.9107142857142857\n",
    "- Holding the autoencoder architecture constant. What would happen to the performance if the label data were not transformed into boolean?\n",
    "    - The model would not be able to distinguish between normal and anomalous data during training if label data were not transformed into boolean\n",
    "- Holding the autoencoder architecture constant. What would the behavior of the error reconstruction if a different optimizer was used? (choose only one optimizer from https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "    - If a different optimizer was used, the behavior of error reconstruction would be affected like its training speed and convergence rate.\n",
    "- Holding the autoencoder architecture constant. Try using binary cross entropy as a loss function and compare its reconstruction error to the original loss function.\n",
    "    - Mean Absolute Error\n",
    "        - Accuracy = 0.946\n",
    "        - Precision = 0.994140625\n",
    "        - Recall = 0.9089285714285714\n",
    "    - Binary Cross Entropy\n",
    "        - Accuracy = 0.946\n",
    "        - Precision = 0.9922178988326849\n",
    "        - Recall = 0.9107142857142857\n",
    "- Try experimenting with the autoencoder architecture and record the result, for which new architecture has the lowest reconstruction error? Try playing around with:\n",
    "    - Activation function: Adam\n",
    "    - Loss function: MAE\n",
    "    - Dropout rate: 0.001\n",
    "    - Units per layer:\n",
    "        - Encoding = 64, 16, 16, 8\n",
    "        - Decoding = 8, 16, 32, 64, 140\n",
    "    - Number of layers:\n",
    "        - Encoding = 5\n",
    "        - Decoding = 5\n",
    "    - Results:\n",
    "        - Accuracy = 0.945\n",
    "        - Precision = 0.9922027290448343\n",
    "        - Recall = 0.9089285714285714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a2c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
